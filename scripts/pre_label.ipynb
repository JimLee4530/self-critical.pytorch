{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import json \n",
    "imgs = json.load(open('/data/common/data/ai_challenger_caption/ai_challenger_caption_train_20170902/caption_train_annotations_20170902.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两个衣着休闲的人在平整的道路上交谈\n",
      "一个穿着红色上衣的男人和一个穿着灰色裤子的男人站在室外的道路上交谈\n",
      "室外的公园里有两个穿着长裤的男人在交流\n",
      "街道上有一个穿着深色外套的男人和一个穿着红色外套的男人在交谈\n",
      "道路上有一个身穿红色上衣的男人在和一个抬着左手的人讲话\n",
      "房间里有三个坐在桌子旁的人在吃饭\n",
      "两个戴着帽子的人和一个短发男人坐在房间里就餐\n",
      "房间里有一个男人和两个戴帽子的老人在吃饭\n",
      "室内三个衣着各异的人坐在桌子旁交谈\n",
      "屋子里有三个衣着各异的人坐在椅子上\n"
     ]
    }
   ],
   "source": [
    "bd = '、'\n",
    "sent_lengths = {}\n",
    "for i in range (2):\n",
    "    for j in range(5):\n",
    "        example = imgs[i]['caption'][j].replace(bd.decode('UTF-8'), '')\n",
    "        seg_list = jieba.cut(example, cut_all=False)\n",
    "        sent_lengths[nw] = sent_lengths.get(nw, 0) + 1\n",
    "        print example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对caption进行分词并保存各词数量和句子长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.322 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time used:', 150.809755)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "bd = '、'\n",
    "counts = {}\n",
    "sent_lengths = {}\n",
    "i = 0\n",
    "start = time.clock()\n",
    "for i in range(210000):\n",
    "    for j in range(5):\n",
    "        example = imgs[i]['caption'][j].replace(bd.decode('UTF-8'), '')\n",
    "        seg_list = jieba.cut(example, cut_all=False)\n",
    "        nw = 0\n",
    "        for w in seg_list:\n",
    "            counts[w]=counts.get(w, 0) + 1\n",
    "            nw = nw + 1\n",
    "        sent_lengths[nw] = sent_lengths.get(nw, 0) + 1\n",
    "cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "max_len = max(sent_lengths.keys()) \n",
    "elapsed = (time.clock() - start)\n",
    "print(\"CUT time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置阈值,查看词汇统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words and their counts:\n",
      "的\n",
      "一个\n",
      "在\n",
      "上\n",
      "男人\n",
      "着\n",
      "穿着\n",
      "有\n",
      "女人\n",
      "两个\n",
      "人\n",
      "拿\n",
      "站\n",
      "里\n",
      "球场上\n",
      "右手\n",
      "戴着\n",
      "走\n",
      "双手\n",
      "道路\n",
      "(1729421, u'\\u7684')\n",
      "(986863, u'\\u4e00\\u4e2a')\n",
      "(862029, u'\\u5728')\n",
      "(582124, u'\\u4e0a')\n",
      "(554635, u'\\u7537\\u4eba')\n",
      "(519252, u'\\u7740')\n",
      "(440255, u'\\u7a7f\\u7740')\n",
      "(414516, u'\\u6709')\n",
      "(411376, u'\\u5973\\u4eba')\n",
      "(271990, u'\\u4e24\\u4e2a')\n",
      "(234585, u'\\u4eba')\n",
      "(228711, u'\\u62ff')\n",
      "(208127, u'\\u7ad9')\n",
      "(176569, u'\\u91cc')\n",
      "(142295, u'\\u7403\\u573a\\u4e0a')\n",
      "(138792, u'\\u53f3\\u624b')\n",
      "(137708, u'\\u6234\\u7740')\n",
      "(128076, u'\\u8d70')\n",
      "(127096, u'\\u53cc\\u624b')\n",
      "(110284, u'\\u9053\\u8def')\n",
      "('total words:', 13825829)\n",
      "17652\n",
      "number of bad words: 9084/17652 = 51.46%\n",
      "number of words in vocab would be 8568\n",
      "number of UNKs: 13654/13825829 = 0.10%\n",
      "inserting the special UNK token\n"
     ]
    }
   ],
   "source": [
    "# 设置词汇出现次数的阈值 查看词典中有多少词汇\n",
    "count_thr = 3\n",
    "print('top words and their counts:')\n",
    "for i in range(20):\n",
    "    print cw[i][1]\n",
    "print('\\n'.join(map(str,cw[:20])))\n",
    "total_words = sum(counts.values())\n",
    "print('total words:', total_words)\n",
    "bad_words = [w for w,n in counts.items() if n <= count_thr]\n",
    "vocab = [w for w,n in counts.items() if n > count_thr]\n",
    "bad_count = sum(counts[w] for w in bad_words)\n",
    "print len(counts)\n",
    "print('number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n",
    "print('number of words in vocab would be %d' % (len(vocab), ))\n",
    "print('number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count*100.0/total_words))\n",
    "\n",
    "# lets now produce the final annotations\n",
    "if bad_count > 0:\n",
    "# additional special UNK token we will use below to map infrequent words to\n",
    "    print('inserting the special UNK token')\n",
    "    vocab.append('UNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看句子长度统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max length sentence in raw data: ', 32)\n",
      "sentence length distribution 累计(count, number of words):\n",
      " 0:          0   0.000000%  0.000000\n",
      " 1:          0   0.000000%  0.000000\n",
      " 2:          0   0.000000%  0.000000\n",
      " 3:          0   0.000000%  0.000000\n",
      " 4:          0   0.000000%  0.000000\n",
      " 5:          3   0.000286%  0.000286\n",
      " 6:         16   0.001524%  0.001810\n",
      " 7:        198   0.018857%  0.020667\n",
      " 8:      17306   1.648190%  1.668857\n",
      " 9:      70067   6.673048%  8.341905\n",
      "10:     125226   11.926286%  20.268190\n",
      "11:     173391   16.513429%  36.781619\n",
      "12:     148898   14.180762%  50.962381\n",
      "13:     118381   11.274381%  62.236762\n",
      "14:      91879   8.750381%  70.987143\n",
      "15:      70236   6.689143%  77.676286\n",
      "16:      60209   5.734190%  83.410476\n",
      "17:      52858   5.034095%  88.444571\n",
      "18:      42799   4.076095%  92.520667\n",
      "19:      31340   2.984762%  95.505429\n",
      "20:      20143   1.918381%  97.423810\n",
      "21:      12140   1.156190%  98.580000\n",
      "22:       6826   0.650095%  99.230095\n",
      "23:       3819   0.363714%  99.593810\n",
      "24:       2025   0.192857%  99.786667\n",
      "25:       1132   0.107810%  99.894476\n",
      "26:        599   0.057048%  99.951524\n",
      "27:        289   0.027524%  99.979048\n",
      "28:        139   0.013238%  99.992286\n",
      "29:         50   0.004762%  99.997048\n",
      "30:         18   0.001714%  99.998762\n",
      "31:         11   0.001048%  99.999810\n",
      "32:          2   0.000190%  100.000000\n"
     ]
    }
   ],
   "source": [
    "print('max length sentence in raw data: ', max_len)\n",
    "print('sentence length distribution 累计(count, number of words):')\n",
    "sum_len = sum(sent_lengths.values())\n",
    "perc = 0\n",
    "for i in range(max_len+1):\n",
    "    perc = perc + sent_lengths.get(i,0)*100.0/sum_len\n",
    "    print('%2d: %10d   %f%%  %f' % (i, sent_lengths.get(i,0), sent_lengths.get(i,0)*100.0/sum_len,perc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# if bad_count > 0:\n",
    "#     print('inserting the special UNK token')\n",
    "#     vocab.append('UNK')\n",
    "start = time.clock()    \n",
    "for i in range(210000):\n",
    "    imgs[i]['final_captions'] = []\n",
    "    for j in range(5):\n",
    "        example = imgs[i]['caption'][j].replace(bd.decode('UTF-8'), '')\n",
    "        seg_list = jieba.cut(example, cut_all=False)\n",
    "        caption = [w if counts.get(w,0) > count_thr else 'UNK' for w in seg_list]\n",
    "        imgs[i]['final_captions'].append(caption)\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Final captions time used:\",elapsed)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8569\n"
     ]
    }
   ],
   "source": [
    "itow = {i+1:w for i,w in enumerate(vocab)} # a 1-indexed vocab translation table\n",
    "\n",
    "wtoi = {w:i+1 for i,w in enumerate(vocab)} # inverse table\n",
    "print wtoi.get('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210000\n",
      "1050000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_length = 22\n",
    "N = len(imgs)\n",
    "M = sum(len(img['final_captions']) for img in imgs) # total number of captions\n",
    "print N\n",
    "print M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'\\u4e24\\u4e2a', u'\\u8863\\u7740', u'\\u4f11\\u95f2', u'\\u7684', u'\\u4eba', u'\\u5728', u'\\u5e73\\u6574', u'\\u7684', u'\\u9053\\u8def', u'\\u4e0a', u'\\u4ea4\\u8c08'], [u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u7ea2\\u8272', u'\\u4e0a\\u8863', u'\\u7684', u'\\u7537\\u4eba', u'\\u548c', u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u7070\\u8272', u'\\u88e4\\u5b50', u'\\u7684', u'\\u7537\\u4eba', u'\\u7ad9', u'\\u5728', u'\\u5ba4\\u5916', u'\\u7684', u'\\u9053\\u8def', u'\\u4e0a', u'\\u4ea4\\u8c08'], [u'\\u5ba4\\u5916', u'\\u7684', u'\\u516c\\u56ed', u'\\u91cc', u'\\u6709', u'\\u4e24\\u4e2a', u'\\u7a7f\\u7740', u'\\u957f\\u88e4', u'\\u7684', u'\\u7537\\u4eba', u'\\u5728', u'\\u4ea4\\u6d41'], [u'\\u8857\\u9053', u'\\u4e0a', u'\\u6709', u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u6df1\\u8272', u'\\u5916\\u5957', u'\\u7684', u'\\u7537\\u4eba', u'\\u548c', u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u7ea2\\u8272', u'\\u5916\\u5957', u'\\u7684', u'\\u7537\\u4eba', u'\\u5728', u'\\u4ea4\\u8c08'], [u'\\u9053\\u8def', u'\\u4e0a', u'\\u6709', u'\\u4e00\\u4e2a', u'\\u8eab\\u7a7f', u'\\u7ea2\\u8272', u'\\u4e0a\\u8863', u'\\u7684', u'\\u7537\\u4eba', u'\\u5728', u'\\u548c', u'\\u4e00\\u4e2a', u'\\u62ac\\u7740', u'\\u5de6\\u624b', u'\\u7684', u'\\u4eba', u'\\u8bb2\\u8bdd']]\n"
     ]
    }
   ],
   "source": [
    "print imgs[0]['final_captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.clock() \n",
    "label_arrays = []\n",
    "label_start_ix = np.zeros(N, dtype='uint32') # note: these will be one-indexed\n",
    "label_end_ix = np.zeros(N, dtype='uint32')\n",
    "label_length = np.zeros(M, dtype='uint32')\n",
    "caption_counter = 0\n",
    "counter = 1\n",
    "for i,img in enumerate(imgs):\n",
    "    n = len(img['final_captions']) \n",
    "    assert n > 0, 'error: some image has no captions'\n",
    "    Li = np.zeros((n, max_length), dtype='uint32')\n",
    "    for j,s in enumerate(img['final_captions']):\n",
    "        label_length[caption_counter] = min(max_length, len(s)) # record the length of this sequence\n",
    "        caption_counter += 1\n",
    "        for k,w in enumerate(s):\n",
    "            if k < max_length:\n",
    "                Li[j,k] = wtoi[w]\n",
    "    \n",
    "    label_arrays.append(Li)\n",
    "    label_start_ix[i] = counter\n",
    "    label_end_ix[i] = counter + n - 1\n",
    "    \n",
    "    counter += n\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Word2index time used:\",elapsed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoded captions to array of size ', (1050000, 22))\n"
     ]
    }
   ],
   "source": [
    "L = np.concatenate(label_arrays, axis=0) # put all the labels together\n",
    "assert L.shape[0] == M, 'lengths don\\'t match? that\\'s weird'\n",
    "assert np.all(label_length > 0), 'error: some caption had no words?'\n",
    "\n",
    "print('encoded captions to array of size ', L.shape)\n",
    "#return L, label_start_ix, label_end_ix, label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output h5 file\n",
    "import h5py\n",
    "N = len(imgs)\n",
    "f_lb = h5py.File('../data/ImageCaption'+'_label.h5', \"w\")\n",
    "f_lb.create_dataset(\"labels\", dtype='uint32', data=L)\n",
    "f_lb.create_dataset(\"label_start_ix\", dtype='uint32', data=label_start_ix)\n",
    "f_lb.create_dataset(\"label_end_ix\", dtype='uint32', data=label_end_ix)\n",
    "f_lb.create_dataset(\"label_length\", dtype='uint32', data=label_length)\n",
    "f_lb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wrote ', 'ImageCaption.json')\n"
     ]
    }
   ],
   "source": [
    "# create output json file\n",
    "import os\n",
    "out = {}\n",
    "out['ix_to_word'] = itow # encode the (1-indexed) vocab\n",
    "out['images'] = []\n",
    "for i,img in enumerate(imgs):\n",
    "\n",
    "    jimg = {}\n",
    "    jimg['split'] = u'train'#img['split']\n",
    "    #if 'filename' in img: jimg['file_path'] = os.path.join(img['filepath'], img['filename']) # copy it over, might need\n",
    "    if 'image_id' in img: jimg['file_path'] = os.path.join(u'caption_train_images_20170902', img['image_id']) # copy it over, might need\n",
    "    if 'image_id' in img: jimg['id'] = img['image_id']\n",
    "    #if 'cocoid' in img: jimg['id'] = img['cocoid'] # copy over & mantain an id, if present (e.g. coco ids, useful)\n",
    "\n",
    "    out['images'].append(jimg)\n",
    "\n",
    "json.dump(out, open('../data/ImageCaption.json', 'w'))\n",
    "print('wrote ', 'ImageCaption.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split': u'train', 'file_path': u'caption_train_images_20170902/8f00f3d0f1008e085ab660e70dffced16a8259f6.jpg'}\n"
     ]
    }
   ],
   "source": [
    "print out['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coco_imgs = json.load(open('data/dataset_coco.json', 'r'))\n",
    "coco_imgs = coco_imgs['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'sentids': [770337, 771687, 772707, 776154, 781998], u'filepath': u'val2014', u'filename': u'COCO_val2014_000000391895.jpg', u'imgid': 0, u'split': u'test', u'sentences': [{u'tokens': [u'a', u'man', u'with', u'a', u'red', u'helmet', u'on', u'a', u'small', u'moped', u'on', u'a', u'dirt', u'road'], u'raw': u'A man with a red helmet on a small moped on a dirt road. ', u'imgid': 0, u'sentid': 770337}, {u'tokens': [u'man', u'riding', u'a', u'motor', u'bike', u'on', u'a', u'dirt', u'road', u'on', u'the', u'countryside'], u'raw': u'Man riding a motor bike on a dirt road on the countryside.', u'imgid': 0, u'sentid': 771687}, {u'tokens': [u'a', u'man', u'riding', u'on', u'the', u'back', u'of', u'a', u'motorcycle'], u'raw': u'A man riding on the back of a motorcycle.', u'imgid': 0, u'sentid': 772707}, {u'tokens': [u'a', u'dirt', u'path', u'with', u'a', u'young', u'person', u'on', u'a', u'motor', u'bike', u'rests', u'to', u'the', u'foreground', u'of', u'a', u'verdant', u'area', u'with', u'a', u'bridge', u'and', u'a', u'background', u'of', u'cloud', u'wreathed', u'mountains'], u'raw': u'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ', u'imgid': 0, u'sentid': 776154}, {u'tokens': [u'a', u'man', u'in', u'a', u'red', u'shirt', u'and', u'a', u'red', u'hat', u'is', u'on', u'a', u'motorcycle', u'on', u'a', u'hill', u'side'], u'raw': u'A man in a red shirt and a red hat is on a motorcycle on a hill side.', u'imgid': 0, u'sentid': 781998}], u'cocoid': 391895}\n",
      "{u'sentids': [681330, 686718, 688839, 693159, 693204], u'filepath': u'val2014', u'filename': u'COCO_val2014_000000522418.jpg', u'imgid': 1, u'split': u'restval', u'sentences': [{u'tokens': [u'a', u'woman', u'wearing', u'a', u'net', u'on', u'her', u'head', u'cutting', u'a', u'cake'], u'raw': u'A woman wearing a net on her head cutting a cake. ', u'imgid': 1, u'sentid': 681330}, {u'tokens': [u'a', u'woman', u'cutting', u'a', u'large', u'white', u'sheet', u'cake'], u'raw': u'A woman cutting a large white sheet cake.', u'imgid': 1, u'sentid': 686718}, {u'tokens': [u'a', u'woman', u'wearing', u'a', u'hair', u'net', u'cutting', u'a', u'large', u'sheet', u'cake'], u'raw': u'A woman wearing a hair net cutting a large sheet cake.', u'imgid': 1, u'sentid': 688839}, {u'tokens': [u'there', u'is', u'a', u'woman', u'that', u'is', u'cutting', u'a', u'white', u'cake'], u'raw': u'there is a woman that is cutting a white cake', u'imgid': 1, u'sentid': 693159}, {u'tokens': [u'a', u'woman', u'marking', u'a', u'cake', u'with', u'the', u'back', u'of', u'a', u'chefs', u'knife'], u'raw': u\"A woman marking a cake with the back of a chef's knife. \", u'imgid': 1, u'sentid': 693204}], u'cocoid': 522418}\n"
     ]
    }
   ],
   "source": [
    "for i,img in enumerate(coco_imgs):\n",
    "    print img\n",
    "    if i == 1:\n",
    "        break\n",
    "    if 'image_id' in img: jimg['file_path'] = os.path.join(u'caption_train_images_20170902', img['image_id']) # copy it over, might need\n",
    "#     jimg = {}\n",
    "#     jimg['split'] = img['split']\n",
    "#     if 'filename' in img: jimg['file_path'] = os.path.join(img['filepath'], img['filename']) # copy it over, might need\n",
    "#     if 'cocoid' in img: jimg['id'] = img['cocoid'] # copy over & mantain an id, if present (e.g. coco ids, useful)\n",
    "\n",
    "# out['images'].append(jimg)\n",
    "\n",
    "json.dump(out, open(params['output_json'], 'w'))\n",
    "print('wrote ', params['output_json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '<eos>']\n",
      "['1 2 3 <eos>']\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "[ref_words = []\n",
    "sent=['1','2','3']\n",
    "tmp_tokens = sent + ['<eos>']\n",
    "print tmp_tokens\n",
    "ref_words.append(' '.join(tmp_tokens))\n",
    "print ref_words\n",
    "for k in xrange(1,5):\n",
    "    print k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'url': u'http://m4.biz.itc.cn/pic/new/n/71/65/Img8296571_n.jpg', u'image_id': u'8f00f3d0f1008e085ab660e70dffced16a8259f6.jpg', 'final_captions': [[u'\\u4e24\\u4e2a', u'\\u8863\\u7740', u'\\u4f11\\u95f2', u'\\u7684', u'\\u4eba', u'\\u5728', u'\\u5e73\\u6574', u'\\u7684', u'\\u9053\\u8def', u'\\u4e0a', u'\\u4ea4\\u8c08'], [u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u7ea2\\u8272', u'\\u4e0a\\u8863', u'\\u7684', u'\\u7537\\u4eba', u'\\u548c', u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u7070\\u8272', u'\\u88e4\\u5b50', u'\\u7684', u'\\u7537\\u4eba', u'\\u7ad9', u'\\u5728', u'\\u5ba4\\u5916', u'\\u7684', u'\\u9053\\u8def', u'\\u4e0a', u'\\u4ea4\\u8c08'], [u'\\u5ba4\\u5916', u'\\u7684', u'\\u516c\\u56ed', u'\\u91cc', u'\\u6709', u'\\u4e24\\u4e2a', u'\\u7a7f\\u7740', u'\\u957f\\u88e4', u'\\u7684', u'\\u7537\\u4eba', u'\\u5728', u'\\u4ea4\\u6d41'], [u'\\u8857\\u9053', u'\\u4e0a', u'\\u6709', u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u6df1\\u8272', u'\\u5916\\u5957', u'\\u7684', u'\\u7537\\u4eba', u'\\u548c', u'\\u4e00\\u4e2a', u'\\u7a7f\\u7740', u'\\u7ea2\\u8272', u'\\u5916\\u5957', u'\\u7684', u'\\u7537\\u4eba', u'\\u5728', u'\\u4ea4\\u8c08'], [u'\\u9053\\u8def', u'\\u4e0a', u'\\u6709', u'\\u4e00\\u4e2a', u'\\u8eab\\u7a7f', u'\\u7ea2\\u8272', u'\\u4e0a\\u8863', u'\\u7684', u'\\u7537\\u4eba', u'\\u5728', u'\\u548c', u'\\u4e00\\u4e2a', u'\\u62ac\\u7740', u'\\u5de6\\u624b', u'\\u7684', u'\\u4eba', u'\\u8bb2\\u8bdd']], u'caption': [u'\\u4e24\\u4e2a\\u8863\\u7740\\u4f11\\u95f2\\u7684\\u4eba\\u5728\\u5e73\\u6574\\u7684\\u9053\\u8def\\u4e0a\\u4ea4\\u8c08', u'\\u4e00\\u4e2a\\u7a7f\\u7740\\u7ea2\\u8272\\u4e0a\\u8863\\u7684\\u7537\\u4eba\\u548c\\u4e00\\u4e2a\\u7a7f\\u7740\\u7070\\u8272\\u88e4\\u5b50\\u7684\\u7537\\u4eba\\u7ad9\\u5728\\u5ba4\\u5916\\u7684\\u9053\\u8def\\u4e0a\\u4ea4\\u8c08', u'\\u5ba4\\u5916\\u7684\\u516c\\u56ed\\u91cc\\u6709\\u4e24\\u4e2a\\u7a7f\\u7740\\u957f\\u88e4\\u7684\\u7537\\u4eba\\u5728\\u4ea4\\u6d41', u'\\u8857\\u9053\\u4e0a\\u6709\\u4e00\\u4e2a\\u7a7f\\u7740\\u6df1\\u8272\\u5916\\u5957\\u7684\\u7537\\u4eba\\u548c\\u4e00\\u4e2a\\u7a7f\\u7740\\u7ea2\\u8272\\u5916\\u5957\\u7684\\u7537\\u4eba\\u5728\\u4ea4\\u8c08', u'\\u9053\\u8def\\u4e0a\\u6709\\u4e00\\u4e2a\\u8eab\\u7a7f\\u7ea2\\u8272\\u4e0a\\u8863\\u7684\\u7537\\u4eba\\u5728\\u548c\\u4e00\\u4e2a\\u62ac\\u7740\\u5de6\\u624b\\u7684\\u4eba\\u8bb2\\u8bdd']}\n"
     ]
    }
   ],
   "source": [
    "print imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
